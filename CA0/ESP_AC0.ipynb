{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf36421",
   "metadata": {},
   "source": [
    "import the libiraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f46ef3-278d-4a46-aada-5c14b9713502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer, Lemmatizer, POSTagger #import the hazm library\n",
    "from hazm import word_tokenize\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "df=pd.read_csv('books_train.csv')#read the data\n",
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "posTagger = POSTagger(model = 'pos_tagger.model',  universal_tag = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1b48a",
   "metadata": {},
   "source": [
    "(replace_bad_characters) function is replace the bad character that defined with blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a5e60b-893e-444e-ae16-c0416bae379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_bad_characters(s):\n",
    "    bad_characters = [';', ':', '!', \"*\", \")\", \"(\", \"؛\", \"\\n\", \",\", \"-\", \".\", \"،\", \"»\", \"«\", \"…\",\"[\", \"]\", \"\\'\" ,\"?\" ]\n",
    "    persian_zero_unicode = ord('۰')\n",
    "    bad_characters += [chr(code) for code in range(persian_zero_unicode, persian_zero_unicode + 10)]\n",
    "    for bad_character in bad_characters:\n",
    "        s = s.replace(bad_character, ' ') \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee6dd8",
   "metadata": {},
   "source": [
    "تابع کلین یک دیتا فریم ورودی میگیرد و آرایه ای از کتاب ها بر میگرداند که در هر کتاب لیستی از کلمات و کتگوری ان کتاب وجود به طور مثال\n",
    "\n",
    "train_data = [\n",
    "    [[\"word1\", \"word2\"], \"category1\"]\n",
    "    [[\"word1\", \"word2\"], \"category2\"]\n",
    "    ...\n",
    "    [[\"word1\", \"word2\"], \"category3\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec8e4e4-c514-44fc-8330-91c5b7d5c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_words(words):#a functio that remove all sentence components except nouns and verbs\n",
    "    word_tags=posTagger.tag(tokens = words)\n",
    "    result=[]\n",
    "    for word_tag in word_tags:\n",
    "        if word_tag[1]=='NOUN' or word_tag[1]=='VERB':\n",
    "            result.append(word_tag[0])\n",
    "    return result\n",
    "\n",
    "def clean_csv_data(df, use_lemmatizer=True, remove_frequent_words=True):\n",
    "    t=[]\n",
    "    for i in range(len(df['title'])):\n",
    "        title = normalizer.normalize(replace_bad_characters(df[\"title\"][i]))\n",
    "        description = normalizer.normalize(replace_bad_characters(df[\"description\"][i]))\n",
    "        t.append( [title + \" \" + description, df[\"categories\"][i]] )\n",
    "\n",
    "    result=[]\n",
    "    for text, category in t:\n",
    "        words=word_tokenize(text)\n",
    "        clean_words=[]\n",
    "        if remove_frequent_words==True:\n",
    "            words=remove_extra_words(words) \n",
    "\n",
    "\n",
    "        for word in words:\n",
    "            if use_lemmatizer==True:\n",
    "                word=lemmatizer.lemmatize(word)\n",
    "\n",
    "            clean_words.append(word)\n",
    "\n",
    "        result.append( [clean_words , category] )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb461af8-79c2-453f-a8af-ba71354f14ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data=clean_csv_data(df, True, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b173cb2",
   "metadata": {},
   "source": [
    "making a list that include all words in title and description that exist in train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161ce30b-a46e-4cbf-8e92-ce55b6eeccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=set()\n",
    "for i in range(len(train_data)):\n",
    "    item=train_data[i]\n",
    "    item_words=item[0]\n",
    "    for j in range(len(item_words)):\n",
    "        item_word=item_words[j]\n",
    "        all_words.add(item_word)\n",
    "    \n",
    "all_words = list(all_words)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0d58b",
   "metadata": {},
   "source": [
    "making a list that include all category that exist in train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7eb6d34-ebc0-40d1-85e8-c97acc47bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = list(set(df[\"categories\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87201e7c-43fb-4285-9aae-62a15276b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(item, array):#a function that fine the index of a item in array\n",
    "    for i in range(len(array)):\n",
    "        if item == array[i]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def make_2d_array(n, m):#a function that make a n*m matrix with zero values\n",
    "    result=[]\n",
    "    for i in range(n):\n",
    "        a=[]\n",
    "        for j in range(m):\n",
    "            a.append(0)\n",
    "        result.append(a)\n",
    "    return result\n",
    "#making a dictionary that cotain word with its index\n",
    "word_indexes = {}\n",
    "for i in range(len(all_words)):\n",
    "    word = all_words[i]\n",
    "    word_indexes[word] = i\n",
    "#making the bow matrix\n",
    "bow=make_2d_array(len(all_categories), len(all_words))\n",
    "for book in train_data:\n",
    "    category=book[1]\n",
    "    category_index=find_index(category, all_categories)\n",
    "    for word in book[0]:\n",
    "        word_index=word_indexes[word]\n",
    "        bow[category_index][word_index]+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd18f8",
   "metadata": {},
   "source": [
    "اینجا ماتریس بوی را تبدیل به احتمال وجود هر کلمه در ان کتگوری میکنیم\n",
    "در ارایه جدید به ازای هر کتگوری یک احتمال برای عدم وجود هم اضافه میکنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d4336a-2d83-4cdc-b104-07a504b0b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probibility_bow = make_2d_array(len(all_categories) , len(all_words) + 1)\n",
    "for i in range(len(probibility_bow)) :\n",
    "    sum_row = 0\n",
    "    \n",
    "    #calculate sum_row\n",
    "    for j in range(len(all_words)):\n",
    "        sum_row += bow[i][j]\n",
    "\n",
    "    for j in range(len(all_words)):\n",
    "        probibility_bow[i][j]=math.log((bow[i][j]+1)/(len(all_words)+1+sum_row)) # Additive Smoothing algorithm\n",
    "    probibility_bow[i].append(math.log(1/(len(all_words)+1+sum_row))) # Additive Smoothing for words that does not exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fde848",
   "metadata": {},
   "source": [
    "  هست که در ان احتمال کلماتی که وجود ندارند صفر در نظر گرفته شده(additive smoothing)قسمت پایین برای چک کردن بدون استفاده از"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f6e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probibility_bow = make_2d_array(len(all_categories) , len(all_words) + 1)\n",
    "# for i in range(len(probibility_bow)) :\n",
    "#     sum_row = 0\n",
    "    \n",
    "#     #calculate sum_row\n",
    "#     for j in range(len(all_words)):\n",
    "#         sum_row += bow[i][j]\n",
    "\n",
    "#     for j in range(len(all_words)):\n",
    "#         if bow[i][j] != 0:\n",
    "#             probibility_bow[i][j]=math.log((bow[i][j])/(sum_row)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4468c",
   "metadata": {},
   "source": [
    "پیدا کردن احتمال هر کتگوری"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290bd5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_probability(category):\n",
    "    tekrar = 0\n",
    "    for book in train_data:\n",
    "        if book[1] == category:\n",
    "            tekrar += 1\n",
    "    return tekrar / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b083507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probability_in_category(category, word):#a functio that return the probability of word in given category from probability_bow\n",
    "    category_index= all_categories.index(category)#find category index in all_category array\n",
    "    if word in word_indexes:\n",
    "        word_index=word_indexes[word]#find word index in all_words dictionary\n",
    "        return probibility_bow[category_index][word_index]\n",
    "    else:\n",
    "        return probibility_bow[category_index][-1]#when the word does not exist in train file\n",
    "        \n",
    "def find_book_category_probability(book , category):#find the probability of each category in each book of test file\n",
    "    probability=0\n",
    "    for word in book[0]:\n",
    "        probability+=word_probability_in_category(category , word )\n",
    "    return probability+math.log(category_probability(category))\n",
    "\n",
    "def find_best_category(book):#a function that find the category with highest probability\n",
    "    best_category = ''\n",
    "    best_probability=None\n",
    "    for i in range(len(all_categories)):\n",
    "        category_probability=find_book_category_probability(book , all_categories[i])\n",
    "        if  best_probability==None or category_probability > best_probability:\n",
    "            best_probability = category_probability\n",
    "            best_category=all_categories[i]\n",
    "    return best_category\n",
    "\n",
    "def run_test(test_data):#this function show the probability of correctness\n",
    "    true_valu=0\n",
    "    for book in test_data:\n",
    "        best_category=find_best_category(book)\n",
    "        if best_category == book[1]:\n",
    "            true_valu +=1\n",
    "            \n",
    "    print(\"the probability of correctness is:\",true_valu/(len(test_data)))\n",
    "    \n",
    "def show_guesses(test_data):\n",
    "    for book in test_data:\n",
    "        print(find_best_category(book))\n",
    "        print(book[1])\n",
    "        print(\"______________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cc54d",
   "metadata": {},
   "source": [
    "clean the test_file and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5531dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability of correctness is: 0.8044444444444444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test=pd.read_csv('books_test.csv') \n",
    "    \n",
    "test_data=clean_csv_data(df_test, True , True)\n",
    "# show_guesses(test_data)\n",
    "run_test(test_data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
